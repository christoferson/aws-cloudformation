AWSTemplateFormatVersion: '2010-09-09'
Description: |
  Multi-Tenant Knowledge Base System - Production-Ready Version

  ARCHITECTURE:
  - Step Functions orchestration with batching (prevents concurrent jobs)
  - Exponential backoff retries (handles transient failures)
  - Dead Letter Queue with auto-recovery (handles persistent failures)
  - X-Ray tracing and structured logging (observability)
  - Cost-optimized S3 lifecycle policies

# ============================================================================
# METADATA
# ============================================================================
Metadata:
  AWS::CloudFormation::Interface:
    ParameterGroups:
      - Label:
          default: "Application Configuration"
        Parameters:
          - ApplicationName
          - Environment
          - AlertEmail

      - Label:
          default: "Ingestion Configuration"
        Parameters:
          - BatchWindowSeconds
          - MaxRetryAttempts

      - Label:
          default: "Vector Configuration"
        Parameters:
          - Dimension
          - DistanceMetric

      - Label:
          default: "Embedding Model Configuration"
        Parameters:
          - EmbeddingModelId
          - EmbeddingModelProvider

      - Label:
          default: "Chunking Configuration"
        Parameters:
          - ChunkingStrategy
          - MaxTokens
          - OverlapPercentage

# ============================================================================
# PARAMETERS
# ============================================================================
Parameters:
  ApplicationName:
    Type: String
    Description: Name of the application
    Default: kb-system
    AllowedPattern: ^[a-z0-9-]+$
    ConstraintDescription: Must contain only lowercase letters, numbers, and hyphens

  Environment:
    Type: String
    Default: dev
    AllowedValues:
      - dev
      - staging
      - prod
    Description: Environment name for resource naming

  AlertEmail:
    Type: String
    Description: Email address for operational alerts (leave empty to disable)
    Default: ''
    AllowedPattern: ^$|^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$
    ConstraintDescription: Must be a valid email address or empty

  # --------------------------------------------------------------------------
  # Ingestion Configuration
  # --------------------------------------------------------------------------

  BatchWindowSeconds:
    Type: Number
    Default: 300
    MinValue: 60
    MaxValue: 900
    Description: |
      Time to wait before triggering Bedrock ingestion (batching window).
      Higher value = more documents per job = lower cost.
      Recommended: 300s (5 minutes)

  MaxRetryAttempts:
    Type: Number
    Default: 3
    MinValue: 1
    MaxValue: 5
    Description: |
      Number of retry attempts before moving to DLQ.
      With exponential backoff: 3 retries = wait up to 14 seconds.

  # --------------------------------------------------------------------------
  # Vector Configuration
  # --------------------------------------------------------------------------

  Dimension:
    Type: Number
    Default: 1024
    AllowedValues:
      - 256
      - 384
      - 512
      - 1024
      - 1536
      - 3072
    Description: Embedding dimension (must match your model)

  DistanceMetric:
    Type: String
    Default: euclidean
    AllowedValues:
      - cosine
      - euclidean
    Description: Distance metric for similarity search

  # --------------------------------------------------------------------------
  # Embedding Model Configuration
  # --------------------------------------------------------------------------

  EmbeddingModelId:
    Type: String
    Default: amazon.titan-embed-text-v2:0
    AllowedValues:
      - amazon.titan-embed-text-v2:0
      - amazon.titan-embed-text-v1
      - amazon.nova-2-multimodal-embeddings-v1:0
      - cohere.embed-english-v3
      - cohere.embed-multilingual-v3
    Description: Bedrock embedding model

  EmbeddingModelProvider:
    Type: String
    Default: amazon
    AllowedValues:
      - amazon
      - cohere
    Description: Embedding model provider

  # --------------------------------------------------------------------------
  # Chunking Configuration
  # --------------------------------------------------------------------------

  ChunkingStrategy:
    Type: String
    Default: FIXED_SIZE
    AllowedValues:
      - FIXED_SIZE
      - NONE
    Description: Document chunking strategy

  MaxTokens:
    Type: Number
    Default: 300
    MinValue: 20
    MaxValue: 8192
    Description: Maximum tokens per chunk

  OverlapPercentage:
    Type: Number
    Default: 20
    MinValue: 0
    MaxValue: 99
    Description: Overlap percentage between chunks

# ============================================================================
# CONDITIONS
# ============================================================================
Conditions:
  UseFixedSizeChunking: !Equals [!Ref ChunkingStrategy, FIXED_SIZE]
  HasAlertEmail: !Not [!Equals [!Ref AlertEmail, '']]

# ============================================================================
# RESOURCES
# ============================================================================
Resources:

  # ==========================================================================
  # SNS ALERTING
  # ==========================================================================

  AlertTopic:
    Type: AWS::SNS::Topic
    Condition: HasAlertEmail
    Properties:
      DisplayName: !Sub '${ApplicationName}-${Environment} Alerts'
      Subscription:
        - Endpoint: !Ref AlertEmail
          Protocol: email
      Tags:
        - Key: Application
          Value: !Ref ApplicationName
        - Key: Environment
          Value: !Ref Environment

  # ==========================================================================
  # S3 VECTOR STORAGE
  # ==========================================================================

  VectorBucket:
    Type: AWS::S3Vectors::VectorBucket
    Properties:
      EncryptionConfiguration:
        SseType: AES256
    # CRITICAL: Retain vectors on stack deletion (expensive to regenerate)
    DeletionPolicy: Retain
    UpdateReplacePolicy: Retain

  SharedVectorIndex:
    Type: AWS::S3Vectors::Index
    Properties:
      VectorBucketArn: !Ref VectorBucket
      DataType: float32
      Dimension: !Ref Dimension
      DistanceMetric: !Ref DistanceMetric

      # CRITICAL: Metadata filtering enables multi-tenancy
      # Non-filterable keys: Large system metadata (performance optimization)
      # All other keys ARE filterable: user_id, kb_id, etc.
      MetadataConfiguration:
        NonFilterableMetadataKeys:
          - AMAZON_BEDROCK_TEXT
          - AMAZON_BEDROCK_METADATA
          - x-amz-bedrock-kb-source-uri
          - x-amz-bedrock-kb-source-file-modality
          - x-amz-bedrock-kb-chunk-id
          - x-amz-bedrock-kb-data-source-id

  # ==========================================================================
  # S3 DOCUMENT STORAGE
  # ==========================================================================

  DocumentBucket:
    Type: AWS::S3::Bucket
    Properties:
      # Enable EventBridge for ingestion trigger
      NotificationConfiguration:
        EventBridgeConfiguration:
          EventBridgeEnabled: true

      # Security: Block public access
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true

      # Encryption at rest
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: AES256
            BucketKeyEnabled: true

      # COST OPTIMIZATION: Lifecycle rules
      LifecycleConfiguration:
        Rules:
          # Move to Intelligent-Tiering after 30 days (auto-optimizes access patterns)
          - Id: MoveToIntelligentTiering
            Status: Enabled
            Transitions:
              - TransitionInDays: 30
                StorageClass: INTELLIGENT_TIERING

          # Cleanup incomplete multipart uploads (prevents hidden costs)
          - Id: CleanupIncompleteUploads
            Status: Enabled
            AbortIncompleteMultipartUpload:
              DaysAfterInitiation: 7

      # Versioning enabled for data protection
      VersioningConfiguration:
        Status: Enabled

      Tags:
        - Key: Application
          Value: !Ref ApplicationName
        - Key: Environment
          Value: !Ref Environment
        - Key: ManagedBy
          Value: CloudFormation

  # Bucket policy: Enforce encryption
  # DocumentBucketPolicy:
  #   Type: AWS::S3::BucketPolicy
  #   Properties:
  #     Bucket: !Ref DocumentBucket
  #     PolicyDocument:
  #       Statement:
  #         - Sid: DenyUnencryptedUploads
  #           Effect: Deny
  #           Principal: '*'
  #           Action: s3:PutObject
  #           Resource: !Sub '${DocumentBucket.Arn}/*'
  #           Condition:
  #             StringNotEquals:
  #               s3:x-amz-server-side-encryption: AES256

  # ==========================================================================
  # SQS QUEUES
  # 
  # PURPOSE: Decouple S3 events from Step Functions, buffer rapid uploads
  # ==========================================================================

  IngestionQueue:
    Type: AWS::SQS::Queue
    Properties:
      FifoQueue: true
      ContentBasedDeduplication: true

      # IMPORTANT: Must be >= Step Functions max execution time
      # 900s = 15 minutes (batch window + job start overhead)
      VisibilityTimeout: 900

      MessageRetentionPeriod: 86400  # 24 hours
      ReceiveMessageWaitTimeSeconds: 20  # Long polling (reduces costs)

      # After MaxRetryAttempts failures → move to DLQ
      RedrivePolicy:
        deadLetterTargetArn: !GetAtt IngestionDeadLetterQueue.Arn
        maxReceiveCount: !Ref MaxRetryAttempts

      Tags:
        - Key: Application
          Value: !Ref ApplicationName
        - Key: Environment
          Value: !Ref Environment

  IngestionDeadLetterQueue:
    Type: AWS::SQS::Queue
    Properties:
      FifoQueue: true
      MessageRetentionPeriod: 1209600  # 14 days for investigation
      Tags:
        - Key: Application
          Value: !Ref ApplicationName
        - Key: Environment
          Value: !Ref Environment

  IngestionQueuePolicy:
    Type: AWS::SQS::QueuePolicy
    Properties:
      Queues:
        - !Ref IngestionQueue
      PolicyDocument:
        Statement:
          - Effect: Allow
            Principal:
              Service: 
                - events.amazonaws.com
                - lambda.amazonaws.com
            Action: sqs:SendMessage
            Resource: !GetAtt IngestionQueue.Arn

  # ==========================================================================
  # DYNAMODB TABLES
  # ==========================================================================

  KnowledgeBaseMetadataTable:
    Type: AWS::DynamoDB::Table
    Properties:
      BillingMode: PAY_PER_REQUEST

      AttributeDefinitions:
        - AttributeName: kb_id
          AttributeType: S
        - AttributeName: user_id
          AttributeType: S
        - AttributeName: created_at
          AttributeType: S

      KeySchema:
        - AttributeName: kb_id
          KeyType: HASH

      # GSI: Query all KBs for a user
      GlobalSecondaryIndexes:
        - IndexName: UserIdIndex
          KeySchema:
            - AttributeName: user_id
              KeyType: HASH
            - AttributeName: created_at
              KeyType: RANGE
          Projection:
            ProjectionType: ALL

      # Point-in-time recovery
      PointInTimeRecoverySpecification:
        PointInTimeRecoveryEnabled: true

      SSESpecification:
        SSEEnabled: true

      StreamSpecification:
        StreamViewType: NEW_AND_OLD_IMAGES

      Tags:
        - Key: Application
          Value: !Ref ApplicationName
        - Key: Environment
          Value: !Ref Environment

  DocumentMetadataTable:
    Type: AWS::DynamoDB::Table
    Properties:
      BillingMode: PAY_PER_REQUEST

      AttributeDefinitions:
        - AttributeName: document_id
          AttributeType: S
        - AttributeName: kb_id
          AttributeType: S
        - AttributeName: uploaded_at
          AttributeType: S

      KeySchema:
        - AttributeName: document_id
          KeyType: HASH

      # GSI: Query all documents in a KB
      GlobalSecondaryIndexes:
        - IndexName: KbIdIndex
          KeySchema:
            - AttributeName: kb_id
              KeyType: HASH
            - AttributeName: uploaded_at
              KeyType: RANGE
          Projection:
            ProjectionType: ALL

      # TTL: Auto-delete old records
      TimeToLiveSpecification:
        AttributeName: ttl
        Enabled: true

      PointInTimeRecoverySpecification:
        PointInTimeRecoveryEnabled: true

      SSESpecification:
        SSEEnabled: true

      Tags:
        - Key: Application
          Value: !Ref ApplicationName
        - Key: Environment
          Value: !Ref Environment

  # ==========================================================================
  # IAM ROLES
  # ==========================================================================

  KnowledgeBaseRole:
    Type: AWS::IAM::Role
    Properties:
      Description: Execution role for Bedrock Knowledge Base

      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: bedrock.amazonaws.com
            Action: sts:AssumeRole
            Condition:
              StringEquals:
                aws:SourceAccount: !Ref AWS::AccountId
              ArnLike:
                aws:SourceArn: !Sub 'arn:aws:bedrock:${AWS::Region}:${AWS::AccountId}:knowledge-base/*'

      Policies:
        # SCOPED: Only the specific embedding model
        - PolicyName: BedrockModelAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Sid: InvokeEmbeddingModel
                Effect: Allow
                Action:
                  - bedrock:InvokeModel
                Resource: !Sub 'arn:aws:bedrock:${AWS::Region}::foundation-model/${EmbeddingModelId}'

        # S3 Document Access
        - PolicyName: S3DocumentAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Sid: ReadDocuments
                Effect: Allow
                Action:
                  - s3:GetObject
                  - s3:ListBucket
                Resource:
                  - !GetAtt DocumentBucket.Arn
                  - !Sub '${DocumentBucket.Arn}/*'

        # S3 Vector Store Access
        - PolicyName: S3VectorAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Sid: VectorOperations
                Effect: Allow
                Action:
                  - s3vectors:PutVectors
                  - s3vectors:GetVectors
                  - s3vectors:DeleteVectors
                  - s3vectors:QueryVectors
                  - s3vectors:ListVectors
                Resource:
                  - !GetAtt VectorBucket.VectorBucketArn
                  - !Sub '${VectorBucket.VectorBucketArn}/*'

      Tags:
        - Key: Application
          Value: !Ref ApplicationName
        - Key: Environment
          Value: !Ref Environment

  LambdaExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      Description: Execution role for ingestion Lambda functions

      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole

      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
        - arn:aws:iam::aws:policy/AWSXRayDaemonWriteAccess

      Policies:
        - PolicyName: SQSAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - sqs:SendMessage
                  - sqs:ReceiveMessage
                  - sqs:DeleteMessage
                  - sqs:GetQueueAttributes
                  - sqs:ChangeMessageVisibility
                Resource:
                  - !GetAtt IngestionQueue.Arn
                  - !GetAtt IngestionDeadLetterQueue.Arn

        - PolicyName: S3Access
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - s3:GetObject
                  - s3:GetObjectMetadata
                  - s3:GetObjectTagging
                Resource: !Sub '${DocumentBucket.Arn}/*'

        - PolicyName: DynamoDBAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - dynamodb:PutItem
                  - dynamodb:UpdateItem
                  - dynamodb:GetItem
                  - dynamodb:Query
                Resource:
                  - !GetAtt DocumentMetadataTable.Arn
                  - !Sub '${DocumentMetadataTable.Arn}/index/*'
                  - !GetAtt KnowledgeBaseMetadataTable.Arn
                  - !Sub '${KnowledgeBaseMetadataTable.Arn}/index/*'

        - PolicyName: StepFunctionsAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - states:StartExecution
                  - states:DescribeExecution
                Resource: !Sub 'arn:aws:states:${AWS::Region}:${AWS::AccountId}:stateMachine:${ApplicationName}-${Environment}-*'

        - PolicyName: CloudWatchMetrics
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - cloudwatch:PutMetricData
                Resource: '*'
                Condition:
                  StringEquals:
                    cloudwatch:namespace: !Sub '${ApplicationName}/${Environment}'

      Tags:
        - Key: Application
          Value: !Ref ApplicationName
        - Key: Environment
          Value: !Ref Environment

  StateMachineExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      Description: Execution role for Ingestion State Machine

      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: states.amazonaws.com
            Action: sts:AssumeRole

      Policies:
        - PolicyName: BedrockIngestionAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - bedrock:StartIngestionJob
                  - bedrock:GetIngestionJob
                  - bedrock:ListIngestionJobs
                Resource: !Sub 'arn:aws:bedrock:${AWS::Region}:${AWS::AccountId}:knowledge-base/*'

        - PolicyName: SQSAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - sqs:SendMessage
                Resource: !GetAtt IngestionDeadLetterQueue.Arn

        - PolicyName: CloudWatchLogs
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - logs:CreateLogDelivery
                  - logs:GetLogDelivery
                  - logs:UpdateLogDelivery
                  - logs:DeleteLogDelivery
                  - logs:ListLogDeliveries
                  - logs:PutResourcePolicy
                  - logs:DescribeResourcePolicies
                  - logs:DescribeLogGroups
                Resource: '*'

        - PolicyName: XRayAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - xray:PutTraceSegments
                  - xray:PutTelemetryRecords
                Resource: '*'

      Tags:
        - Key: Application
          Value: !Ref ApplicationName
        - Key: Environment
          Value: !Ref Environment

  # ==========================================================================
  # BEDROCK KNOWLEDGE BASE
  # ==========================================================================

  SharedKnowledgeBase:
    Type: AWS::Bedrock::KnowledgeBase
    Properties:
      Name: !Sub '${ApplicationName}-${Environment}-multi-tenant-kb'
      Description: !Sub 'Shared multi-tenant knowledge base for ${ApplicationName}'
      RoleArn: !GetAtt KnowledgeBaseRole.Arn

      KnowledgeBaseConfiguration:
        Type: VECTOR
        VectorKnowledgeBaseConfiguration:
          EmbeddingModelArn: !Sub 'arn:aws:bedrock:${AWS::Region}::foundation-model/${EmbeddingModelId}'
          EmbeddingModelConfiguration:
            BedrockEmbeddingModelConfiguration:
              Dimensions: !Ref Dimension

      StorageConfiguration:
        Type: S3_VECTORS
        S3VectorsConfiguration:
          VectorBucketArn: !GetAtt VectorBucket.VectorBucketArn
          IndexArn: !GetAtt SharedVectorIndex.IndexArn

      Tags:
        Application: !Ref ApplicationName
        Environment: !Ref Environment
        ManagedBy: CloudFormation

  SharedDataSource:
    Type: AWS::Bedrock::DataSource
    Properties:
      Name: !Sub '${ApplicationName}-${Environment}-multi-tenant-kb-ds'
      Description: Shared S3 data source for all users
      KnowledgeBaseId: !Ref SharedKnowledgeBase

      DataSourceConfiguration:
        Type: S3
        S3Configuration:
          BucketArn: !GetAtt DocumentBucket.Arn
          # No inclusionPrefixes - scan entire bucket, filter at query time

      VectorIngestionConfiguration:
        ChunkingConfiguration:
          ChunkingStrategy: !Ref ChunkingStrategy
          FixedSizeChunkingConfiguration: !If
            - UseFixedSizeChunking
            - MaxTokens: !Ref MaxTokens
              OverlapPercentage: !Ref OverlapPercentage
            - !Ref AWS::NoValue

  # ==========================================================================
  # STEP FUNCTIONS STATE MACHINE (SIMPLIFIED)
  # 
  # FLOW: Batch Window → Check Job → Start Job with Exponential Backoff → DLQ
  # NO CIRCUIT BREAKER: Just retries with backoff, then DLQ
  # ==========================================================================

  IngestionStateMachineLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub '/aws/vendedlogs/states/${ApplicationName}-${Environment}-ingestion-sm'
      RetentionInDays: 30

  IngestionStateMachine:
    Type: AWS::StepFunctions::StateMachine
    Properties:
      StateMachineName: !Sub '${ApplicationName}-${Environment}-ingestion-sm'
      StateMachineType: STANDARD
      RoleArn: !GetAtt StateMachineExecutionRole.Arn

      # X-Ray tracing enabled
      TracingConfiguration:
        Enabled: true

      # CloudWatch Logs enabled
      LoggingConfiguration:
        Level: ALL
        IncludeExecutionData: true
        Destinations:
          - CloudWatchLogsLogGroup:
              LogGroupArn: !GetAtt IngestionStateMachineLogGroup.Arn

      DefinitionString: !Sub |
        {
          "Comment": "Batched Bedrock Ingestion with Exponential Backoff",
          "StartAt": "WaitForBatchWindow",
          "States": {

            "WaitForBatchWindow": {
              "Type": "Wait",
              "Seconds": ${BatchWindowSeconds},
              "Comment": "Accumulate multiple uploads into a single job (cost optimization)",
              "Next": "CheckIfJobRunning"
            },

            "CheckIfJobRunning": {
              "Type": "Task",
              "Resource": "arn:aws:states:::aws-sdk:bedrockagent:listIngestionJobs",
              "Comment": "Check if another ingestion job is already running",
              "Parameters": {
                "KnowledgeBaseId": "${SharedKnowledgeBase}",
                "DataSourceId": "${SharedDataSource.DataSourceId}",
                "MaxResults": 1
              },
              "ResultPath": "$.jobCheck",
              "Next": "EvaluateJobStatus",
              "Retry": [
                {
                  "ErrorEquals": ["States.TaskFailed", "States.Timeout"],
                  "IntervalSeconds": 2,
                  "MaxAttempts": 3,
                  "BackoffRate": 2.0,
                  "Comment": "Exponential backoff for API errors"
                }
              ],
              "Catch": [
                {
                  "ErrorEquals": ["States.ALL"],
                  "Next": "SendToDLQ",
                  "ResultPath": "$.error"
                }
              ]
            },

            "EvaluateJobStatus": {
              "Type": "Choice",
              "Comment": "Decide whether to wait or start a new job",
              "Choices": [
                {
                  "And": [
                    {"Variable": "$.jobCheck.IngestionJobSummaries[0]", "IsPresent": true},
                    {"Variable": "$.jobCheck.IngestionJobSummaries[0].Status", "StringEquals": "IN_PROGRESS"}
                  ],
                  "Next": "WaitForJobCompletion"
                },
                {
                  "And": [
                    {"Variable": "$.jobCheck.IngestionJobSummaries[0]", "IsPresent": true},
                    {"Variable": "$.jobCheck.IngestionJobSummaries[0].Status", "StringEquals": "STARTING"}
                  ],
                  "Next": "WaitForJobCompletion"
                }
              ],
              "Default": "StartIngestionJob"
            },

            "WaitForJobCompletion": {
              "Type": "Wait",
              "Seconds": 60,
              "Comment": "Job is running, wait 1 minute before checking again",
              "Next": "CheckIfJobRunning"
            },

            "StartIngestionJob": {
              "Type": "Task",
              "Resource": "arn:aws:states:::aws-sdk:bedrockagent:startIngestionJob",
              "Comment": "Start Bedrock ingestion job for all pending documents",
              "Parameters": {
                "KnowledgeBaseId": "${SharedKnowledgeBase}",
                "DataSourceId": "${SharedDataSource.DataSourceId}",
                "Description.$": "States.Format('Batch ingestion at {}', $$.State.EnteredTime)"
              },
              "ResultPath": "$.ingestionJob",
              "End": true,
              "Retry": [
                {
                  "ErrorEquals": ["ConflictException"],
                  "IntervalSeconds": 30,
                  "MaxAttempts": 5,
                  "BackoffRate": 2.0,
                  "Comment": "Another job started between check and start - exponential backoff: 30s, 60s, 120s, 240s, 480s"
                },
                {
                  "ErrorEquals": ["ThrottlingException", "ServiceQuotaExceededException"],
                  "IntervalSeconds": 60,
                  "MaxAttempts": 3,
                  "BackoffRate": 2.0,
                  "Comment": "Bedrock API throttling - aggressive backoff: 60s, 120s, 240s"
                },
                {
                  "ErrorEquals": ["States.ALL"],
                  "IntervalSeconds": 10,
                  "MaxAttempts": 2,
                  "BackoffRate": 2.0,
                  "Comment": "General errors - short backoff: 10s, 20s"
                }
              ],
              "Catch": [
                {
                  "ErrorEquals": ["States.ALL"],
                  "Next": "SendToDLQ",
                  "ResultPath": "$.error"
                }
              ]
            },

            "SendToDLQ": {
              "Type": "Task",
              "Resource": "arn:aws:states:::sqs:sendMessage",
              "Comment": "All retries exhausted - send to DLQ for manual investigation or auto-recovery",
              "Parameters": {
                "QueueUrl": "${IngestionDeadLetterQueue}",
                "MessageBody": {
                  "error.$": "$.error",
                  "input.$": "$",
                  "executionId.$": "$$.Execution.Id",
                  "timestamp.$": "$$.State.EnteredTime"
                },
                "MessageGroupId": "dlq-messages"
              },
              "ResultPath": "$.dlqResult",
              "Next": "FailState"
            },

            "FailState": {
              "Type": "Fail",
              "Error": "IngestionFailed",
              "Cause": "Failed to start ingestion job after exponential backoff retries. Check DLQ for details."
            }
          }
        }

  # ==========================================================================
  # LAMBDA FUNCTIONS
  # ==========================================================================

  IngestionTriggerLambda:
    Type: AWS::Lambda::Function
    Properties:
      Description: Parse S3 events and trigger ingestion pipeline
      Runtime: python3.12
      Handler: index.lambda_handler
      Role: !GetAtt LambdaExecutionRole.Arn
      Timeout: 30
      MemorySize: 128

      TracingConfig:
        Mode: Active

      Environment:
        Variables:
          STATE_MACHINE_ARN: !Ref IngestionStateMachine
          DOCUMENT_METADATA_TABLE: !Ref DocumentMetadataTable
          KB_METADATA_TABLE: !Ref KnowledgeBaseMetadataTable
          APPLICATION_NAME: !Ref ApplicationName
          ENVIRONMENT: !Ref Environment

      Code:
        ZipFile: |
          import json
          import boto3
          import os
          from datetime import datetime
          from urllib.parse import unquote_plus

          stepfunctions = boto3.client('stepfunctions')
          dynamodb = boto3.resource('dynamodb')
          s3 = boto3.client('s3')
          cloudwatch = boto3.client('cloudwatch')

          STATE_MACHINE_ARN = os.environ['STATE_MACHINE_ARN']
          DOCUMENT_METADATA_TABLE = os.environ['DOCUMENT_METADATA_TABLE']
          KB_METADATA_TABLE = os.environ['KB_METADATA_TABLE']
          APP_NAME = os.environ['APPLICATION_NAME']
          ENV = os.environ['ENVIRONMENT']

          def lambda_handler(event, context):
              print(json.dumps({
                  'event': 'function_invoked',
                  'request_id': context.aws_request_id
              }))

              try:
                  detail = event.get('detail', {})
                  bucket = detail.get('bucket', {}).get('name')
                  key = unquote_plus(detail.get('object', {}).get('key', ''))
                  size = detail.get('object', {}).get('size', 0)

                  if not bucket or not key:
                      print(json.dumps({'event': 'invalid_event', 'reason': 'missing_bucket_or_key'}))
                      return {'statusCode': 400, 'body': 'Invalid event'}

                  if key.endswith('.metadata.json'):
                      print(json.dumps({'event': 'skipped_metadata_file', 'key': key}))
                      return {'statusCode': 200, 'body': 'Skipped metadata file'}

                  parts = key.split('/')
                  if len(parts) < 3:
                      print(json.dumps({'event': 'invalid_path', 'key': key}))
                      return {'statusCode': 400, 'body': 'Path must be {user_id}/{kb_id}/{filename}'}

                  user_id = parts[0]
                  kb_id = parts[1]
                  filename = '/'.join(parts[2:])
                  document_id = f"{user_id}_{kb_id}_{filename}".replace('/', '_')

                  print(json.dumps({
                      'event': 'document_parsed',
                      'user_id': user_id,
                      'kb_id': kb_id,
                      'document_id': document_id,
                      'file_size': size
                  }))

                  custom_metadata = {}
                  try:
                      obj_metadata = s3.head_object(Bucket=bucket, Key=key)
                      custom_metadata = obj_metadata.get('Metadata', {})
                  except Exception as e:
                      print(json.dumps({'event': 'metadata_fetch_warning', 'error': str(e)}))

                  doc_table = dynamodb.Table(DOCUMENT_METADATA_TABLE)
                  item = {
                      'document_id': document_id,
                      'kb_id': kb_id,
                      'user_id': user_id,
                      'filename': filename,
                      's3_key': key,
                      's3_bucket': bucket,
                      'file_size': size,
                      'uploaded_at': datetime.utcnow().isoformat(),
                      'ingestion_status': 'QUEUED'
                  }

                  if custom_metadata:
                      item['custom_metadata'] = custom_metadata

                  doc_table.put_item(Item=item)

                  print(json.dumps({'event': 'metadata_saved', 'document_id': document_id}))

                  execution_name = f"{kb_id}-{int(datetime.utcnow().timestamp() * 1000)}"

                  try:
                      response = stepfunctions.start_execution(
                          stateMachineArn=STATE_MACHINE_ARN,
                          name=execution_name,
                          input=json.dumps({
                              'kb_id': kb_id,
                              'user_id': user_id,
                              'document_id': document_id,
                              's3_key': key,
                              's3_bucket': bucket,
                              'file_size': size,
                              'trigger': 'S3_UPLOAD',
                              'timestamp': datetime.utcnow().isoformat()
                          })
                      )

                      execution_arn = response['executionArn']

                      print(json.dumps({
                          'event': 'execution_started',
                          'execution_arn': execution_arn,
                          'execution_name': execution_name,
                          'document_id': document_id
                      }))

                      try:
                          cloudwatch.put_metric_data(
                              Namespace=f'{APP_NAME}/{ENV}',
                              MetricData=[{
                                  'MetricName': 'DocumentsQueued',
                                  'Value': 1,
                                  'Unit': 'Count',
                                  'Dimensions': [
                                      {'Name': 'KnowledgeBaseId', 'Value': kb_id},
                                      {'Name': 'UserId', 'Value': user_id}
                                  ]
                              }]
                          )
                      except Exception as e:
                          print(json.dumps({'event': 'metrics_warning', 'error': str(e)}))

                      return {
                          'statusCode': 200,
                          'body': json.dumps({
                              'document_id': document_id,
                              'execution_arn': execution_arn,
                              'status': 'QUEUED'
                          })
                      }

                  except stepfunctions.exceptions.ExecutionAlreadyExists:
                      print(json.dumps({'event': 'execution_already_exists', 'execution_name': execution_name}))
                      return {'statusCode': 200, 'body': 'Already processing'}

              except Exception as e:
                  print(json.dumps({
                      'event': 'error',
                      'error_type': type(e).__name__,
                      'error_message': str(e),
                      'request_id': context.aws_request_id
                  }))
                  raise

      Tags:
        - Key: Application
          Value: !Ref ApplicationName
        - Key: Environment
          Value: !Ref Environment

  IngestionTriggerLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub '/aws/lambda/${IngestionTriggerLambda}'
      RetentionInDays: 30

  DLQProcessorLambda:
    Type: AWS::Lambda::Function
    Properties:
      Description: Auto-recovery Lambda for DLQ messages
      Runtime: python3.12
      Handler: index.lambda_handler
      Role: !GetAtt LambdaExecutionRole.Arn
      Timeout: 300
      MemorySize: 256
      ReservedConcurrentExecutions: 1

      TracingConfig:
        Mode: Active

      Environment:
        Variables:
          DLQ_URL: !Ref IngestionDeadLetterQueue
          STATE_MACHINE_ARN: !Ref IngestionStateMachine
          SNS_TOPIC_ARN: !If [HasAlertEmail, !Ref AlertTopic, '']
          MAX_RETRY_AGE_HOURS: '24'

      Code:
        ZipFile: |
          import json
          import boto3
          import os
          from datetime import datetime, timedelta

          sqs = boto3.client('sqs')
          stepfunctions = boto3.client('stepfunctions')
          sns = boto3.client('sns')

          DLQ_URL = os.environ['DLQ_URL']
          STATE_MACHINE_ARN = os.environ['STATE_MACHINE_ARN']
          SNS_TOPIC_ARN = os.environ.get('SNS_TOPIC_ARN', '')
          MAX_RETRY_AGE_HOURS = int(os.environ.get('MAX_RETRY_AGE_HOURS', 24))

          def lambda_handler(event, context):
              print(json.dumps({
                  'event': 'dlq_processor_started',
                  'request_id': context.aws_request_id
              }))

              try:
                  response = sqs.receive_message(
                      QueueUrl=DLQ_URL,
                      MaxNumberOfMessages=10,
                      WaitTimeSeconds=5
                  )

                  messages = response.get('Messages', [])

                  if not messages:
                      print(json.dumps({'event': 'dlq_empty'}))
                      return {'statusCode': 200, 'body': 'No messages in DLQ'}

                  print(json.dumps({'event': 'messages_received', 'count': len(messages)}))

                  recovered_count = 0
                  expired_count = 0
                  failed_count = 0

                  for message in messages:
                      try:
                          body = json.loads(message['Body'])
                          receipt_handle = message['ReceiptHandle']

                          if 'input' in body:
                              original_input = body['input']
                          else:
                              original_input = body

                          timestamp_str = original_input.get('timestamp', '')
                          if timestamp_str:
                              try:
                                  timestamp = datetime.fromisoformat(timestamp_str.replace('Z', ''))
                                  age_hours = (datetime.utcnow() - timestamp).total_seconds() / 3600

                                  if age_hours > MAX_RETRY_AGE_HOURS:
                                      print(json.dumps({
                                          'event': 'message_expired',
                                          'age_hours': age_hours,
                                          'document_id': original_input.get('document_id')
                                      }))

                                      sqs.delete_message(QueueUrl=DLQ_URL, ReceiptHandle=receipt_handle)
                                      expired_count += 1
                                      continue
                              except ValueError:
                                  pass

                          kb_id = original_input.get('kb_id', 'unknown')
                          execution_name = f"{kb_id}-retry-{int(datetime.utcnow().timestamp() * 1000)}"

                          stepfunctions.start_execution(
                              stateMachineArn=STATE_MACHINE_ARN,
                              name=execution_name,
                              input=json.dumps(original_input)
                          )

                          sqs.delete_message(QueueUrl=DLQ_URL, ReceiptHandle=receipt_handle)

                          recovered_count += 1

                          print(json.dumps({
                              'event': 'message_recovered',
                              'kb_id': kb_id,
                              'document_id': original_input.get('document_id'),
                              'execution_name': execution_name
                          }))

                      except Exception as e:
                          failed_count += 1
                          print(json.dumps({
                              'event': 'recovery_failed',
                              'error_type': type(e).__name__,
                              'error_message': str(e),
                              'message_id': message.get('MessageId')
                          }))

                  if failed_count > 0 and SNS_TOPIC_ARN:
                      try:
                          sns.publish(
                              TopicArn=SNS_TOPIC_ARN,
                              Subject=f'DLQ Recovery Issues - {failed_count} messages failed',
                              Message=json.dumps({
                                  'recovered': recovered_count,
                                  'expired': expired_count,
                                  'failed': failed_count,
                                  'timestamp': datetime.utcnow().isoformat()
                              }, indent=2)
                          )
                      except Exception as e:
                          print(json.dumps({'event': 'sns_alert_failed', 'error': str(e)}))

                  result = {
                      'recovered': recovered_count,
                      'expired': expired_count,
                      'failed': failed_count
                  }

                  print(json.dumps({'event': 'dlq_processor_completed', 'result': result}))

                  return {'statusCode': 200, 'body': json.dumps(result)}

              except Exception as e:
                  print(json.dumps({
                      'event': 'dlq_processor_error',
                      'error_type': type(e).__name__,
                      'error_message': str(e)
                  }))
                  raise

      Tags:
        - Key: Application
          Value: !Ref ApplicationName
        - Key: Environment
          Value: !Ref Environment

  DLQProcessorLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub '/aws/lambda/${DLQProcessorLambda}'
      RetentionInDays: 30

  # ==========================================================================
  # EVENTBRIDGE RULES
  # ==========================================================================

  IngestionTriggerEventsRule:
    Type: AWS::Events::Rule
    Properties:
      Description: Trigger ingestion Lambda when documents are uploaded to S3
      State: ENABLED

      EventPattern:
        source:
          - aws.s3
        detail-type:
          - Object Created
        detail:
          bucket:
            name:
              - !Ref DocumentBucket

      Targets:
        - Arn: !GetAtt IngestionTriggerLambda.Arn
          Id: IngestionTriggerLambdaTarget
          RetryPolicy:
            MaximumRetryAttempts: 2
            MaximumEventAgeInSeconds: 3600

  IngestionTriggerLambdaEventBridgePermission:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !Ref IngestionTriggerLambda
      Action: lambda:InvokeFunction
      Principal: events.amazonaws.com
      SourceArn: !GetAtt IngestionTriggerEventsRule.Arn

  # DLQ Processor Schedule - runs every 15 minutes
  DLQProcessorSchedule:
    Type: AWS::Events::Rule
    Properties:
      Description: Auto-recover messages from DLQ every 15 minutes
      State: ENABLED
      ScheduleExpression: rate(15 minutes)
      Targets:
        - Arn: !GetAtt DLQProcessorLambda.Arn
          Id: DLQProcessorTarget

  DLQProcessorLambdaSchedulePermission:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !Ref DLQProcessorLambda
      Action: lambda:InvokeFunction
      Principal: events.amazonaws.com
      SourceArn: !GetAtt DLQProcessorSchedule.Arn

  # EventBridge Archive - 7 day replay capability
  S3EventArchive:
    Type: AWS::Events::Archive
    Properties:
      ArchiveName: !Sub '${ApplicationName}-${Environment}-s3-events-archive'
      Description: Archive of S3 upload events for replay capability
      SourceArn: !Sub 'arn:aws:events:${AWS::Region}:${AWS::AccountId}:event-bus/default'
      RetentionDays: 7
      EventPattern:
        source:
          - aws.s3
        detail-type:
          - Object Created
        detail:
          bucket:
            name:
              - !Ref DocumentBucket

  # ==========================================================================
  # CLOUDWATCH ALARMS
  # ==========================================================================

  DLQMessagesAlarm:
    Type: AWS::CloudWatch::Alarm
    Condition: HasAlertEmail
    Properties:
      AlarmName: !Sub '${ApplicationName}-${Environment}-dlq-has-messages'
      AlarmDescription: Alert when messages appear in DLQ
      MetricName: ApproximateNumberOfMessagesVisible
      Namespace: AWS/SQS
      Statistic: Sum
      Period: 300
      EvaluationPeriods: 1
      Threshold: 1
      ComparisonOperator: GreaterThanOrEqualToThreshold
      Dimensions:
        - Name: QueueName
          Value: !GetAtt IngestionDeadLetterQueue.QueueName
      AlarmActions:
        - !Ref AlertTopic
      TreatMissingData: notBreaching

  StateMachineFailuresAlarm:
    Type: AWS::CloudWatch::Alarm
    Condition: HasAlertEmail
    Properties:
      AlarmName: !Sub '${ApplicationName}-${Environment}-step-functions-failures'
      AlarmDescription: Alert on Step Functions execution failures
      MetricName: ExecutionsFailed
      Namespace: AWS/States
      Statistic: Sum
      Period: 300
      EvaluationPeriods: 1
      Threshold: 3
      ComparisonOperator: GreaterThanThreshold
      Dimensions:
        - Name: StateMachineArn
          Value: !Ref IngestionStateMachine
      AlarmActions:
        - !Ref AlertTopic
      TreatMissingData: notBreaching

  LambdaErrorsAlarm:
    Type: AWS::CloudWatch::Alarm
    Condition: HasAlertEmail
    Properties:
      AlarmName: !Sub '${ApplicationName}-${Environment}-lambda-errors'
      AlarmDescription: Alert on Lambda function errors
      MetricName: Errors
      Namespace: AWS/Lambda
      Statistic: Sum
      Period: 300
      EvaluationPeriods: 1
      Threshold: 5
      ComparisonOperator: GreaterThanThreshold
      Dimensions:
        - Name: FunctionName
          Value: !Ref IngestionTriggerLambda
      AlarmActions:
        - !Ref AlertTopic
      TreatMissingData: notBreaching

  QueueAgeAlarm:
    Type: AWS::CloudWatch::Alarm
    Condition: HasAlertEmail
    Properties:
      AlarmName: !Sub '${ApplicationName}-${Environment}-queue-age-high'
      AlarmDescription: Alert when messages are stuck in queue
      MetricName: ApproximateAgeOfOldestMessage
      Namespace: AWS/SQS
      Statistic: Maximum
      Period: 300
      EvaluationPeriods: 2
      Threshold: 1800
      ComparisonOperator: GreaterThanThreshold
      Dimensions:
        - Name: QueueName
          Value: !GetAtt IngestionQueue.QueueName
      AlarmActions:
        - !Ref AlertTopic
      TreatMissingData: notBreaching

# ============================================================================
# OUTPUTS
# ============================================================================
Outputs:

  VectorBucketName:
    Description: Name of the S3 Vector Bucket
    Value: !Ref VectorBucket

  VectorBucketArn:
    Description: ARN of the S3 Vector Bucket
    Value: !GetAtt VectorBucket.VectorBucketArn

  VectorIndexArn:
    Description: ARN of the shared Vector Index
    Value: !GetAtt SharedVectorIndex.IndexArn

  DocumentBucketName:
    Description: Name of the S3 Document Bucket
    Value: !Ref DocumentBucket

  DocumentBucketArn:
    Description: ARN of the S3 Document Bucket
    Value: !GetAtt DocumentBucket.Arn

  IngestionQueueUrl:
    Description: URL of the ingestion queue
    Value: !Ref IngestionQueue

  IngestionDeadLetterQueueUrl:
    Description: URL of the dead letter queue
    Value: !Ref IngestionDeadLetterQueue

  KnowledgeBaseMetadataTableName:
    Description: Name of the KB Metadata DynamoDB table
    Value: !Ref KnowledgeBaseMetadataTable

  DocumentMetadataTableName:
    Description: Name of the Document Metadata DynamoDB table
    Value: !Ref DocumentMetadataTable

  KnowledgeBaseId:
    Description: ID of the shared Bedrock Knowledge Base
    Value: !Ref SharedKnowledgeBase

  KnowledgeBaseArn:
    Description: ARN of the shared Bedrock Knowledge Base
    Value: !GetAtt SharedKnowledgeBase.KnowledgeBaseArn

  DataSourceId:
    Description: ID of the shared Data Source
    Value: !GetAtt SharedDataSource.DataSourceId

  StateMachineArn:
    Description: ARN of the Ingestion Step Functions State Machine
    Value: !Ref IngestionStateMachine

  IngestionTriggerLambdaArn:
    Description: ARN of the ingestion trigger Lambda function
    Value: !GetAtt IngestionTriggerLambda.Arn

  DLQProcessorLambdaArn:
    Description: ARN of the DLQ processor Lambda function
    Value: !GetAtt DLQProcessorLambda.Arn

  AlertTopicArn:
    Condition: HasAlertEmail
    Description: ARN of the SNS alert topic
    Value: !Ref AlertTopic

  ConfigurationSummary:
    Description: Configuration summary
    Value: !Sub |
      ===================================================
      ${ApplicationName} - ${Environment}
      ===================================================

      STORAGE:
        Document Bucket: ${DocumentBucket}
        Vector Bucket: ${VectorBucket}
        Path: s3://${DocumentBucket}/{user_id}/{kb_id}/{documents}

      KNOWLEDGE BASE:
        KB ID: ${SharedKnowledgeBase}
        Data Source ID: ${SharedDataSource.DataSourceId}
        Model: ${EmbeddingModelId}
        Dimension: ${Dimension}

      QUEUES:
        Main Queue: ${IngestionQueue}
        DLQ: ${IngestionDeadLetterQueue}

      ORCHESTRATION:
        State Machine: ${IngestionStateMachine}
        Batch Window: ${BatchWindowSeconds}s
        Max Retries: ${MaxRetryAttempts}
        DLQ Auto-Recovery: Every 15 minutes

      MONITORING:
        X-Ray Tracing: Enabled
        CloudWatch Alarms: ${AlertEmail}
        Event Archive: 7 days

  QuickStartGuide:
    Description: Quick start guide
    Value: !Sub |
      ===================================================
      QUICK START
      ===================================================

      1. UPLOAD:
         aws s3 cp doc.pdf s3://${DocumentBucket}/{user_id}/{kb_id}/

      2. FLOW:
         Upload → EventBridge → Lambda → Step Functions
         → Batch Window (${BatchWindowSeconds}s)
         → Bedrock Ingestion (with exponential backoff)
         → On failure: Retry → DLQ → Auto-recovery (15min)

      3. QUERY (with metadata filtering):
         bedrock-agent-runtime retrieve-and-generate \
           --knowledge-base-id ${SharedKnowledgeBase} \
           --retrieval-configuration '{
             "vectorSearchConfiguration": {
               "filter": {"equals": {"key": "user_id", "value": "user123"}}
             }
           }' \
           --input '{"text": "Your question?"}'

      4. MONITORING:
         - DLQ messages trigger email alerts
         - Auto-recovery runs every 15 minutes
         - View traces: X-Ray console
         - View metrics: CloudWatch console